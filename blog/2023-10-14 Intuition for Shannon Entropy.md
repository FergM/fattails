# About
An article outlining the intuition behind Shannon Entropy and how it works for information theory.

Article Status: **Roughwork**

Sections
1. Definition of Shannon Entropy
2. Behaviour of the summand expression
3. Extension to more dimensions

## 1. Definition of Shannon Entropy
Definition of [Shannon Entropy](https://en.wikipedia.org/wiki/Entropy_(information_theory))
:
  
  ![Shannon Entropy Definition](./images/2023-10-16%20Shannon%20Entropy.png)

* As a convention I will use the term **"*summand function*"** for the following expression:

    ![Shannon Entropy Summand](./images/2023-10-16%20Shannon%20Entropy%20Summand.png)
    
* **I will also propose alternative *summand functions*** which achieve the same outcome

### Alternative Definitions of the *summand function*:
* We can define the summand function as the product of p with any generic function of p, say *f(p)*, as long as the function *f(p)* is concave (negative second derivative)
* We can replace log(p) with any of the following and still get similar results
    * p
    * p^2 (or even p^n where n is any integer) **
    * e^p
    * r^p where r is a positive real number
    * p^r where r is a positive real number
* In fact typically the result will be the exact same, I need to check for constrained multidimensional cases where the choice of *summand function* affects the result

*\*\* Might also work for looser conditions*

Side-note: Interestingly entropy in processes is force times flux. flux is often itself proportional to force (e.g. heat transfer rates are proprtional to temperature differences), so you have an equation that looks like deltaT times f(deltaT), and f(deltaT) is monotonic increasing. So you end up with the same mathematical formulation as p f(p). I'm not sure if that's really related to probabilities of states, but probably is.

## 2. Behaviour of the summand expression
This is the key part you need to understand.

1. What you want to do is plot the *summand function*
2. Then you'll see that you want a concave *summand function*

Why?
* Want the `p_i == p_j` etc. solution to have the maximum entropy, i.e. all states are equally likely.
* That is, the maximum entropy should happen if there is equilibrium and you have no further information

What next
* You start applying constraints to the values of p you want to accept [suggestion: consider whether these are constraints on the values of p, or, perhaps, aggregate observations (e.g. an average)]
* This restricts p and then from here you can look and see what available solution has the max entropy [side-note: 2nd law of thermodynamics says that entropy in a closed system can only increase, this is the rationale why eventually equilibrium hits max entropy, which is when there is equipartition among states]
* And within the available solutions you want to continue this logic of *"penalising discrimination"*

Roughwork comment
* I'm going to be hand wavy here for now

Conclusion
* Basically if your *summand function* is concave, then the average of two different points on the function is *"lower"* than the value of the function if you *"move"* the p_heads and p_tails values to equal oneanother
* More generally this concavity uses *Jensen's Inequality* to prioritise solutions which keep p_i and p_j apart from each other

Application
* So as an application, you add constraints which resemble the problem you want to solve
* Then you use entropy and look at the space of options which have not been excluded through constraints
* And you select amongst the remaining options using entropy
* Which really means, you select by picking the least descriminatory option(s), and Shannon Entropy (or any convex *summand function*) through it's concavity applies this *principle of non discrimination* mathematically

Mathematica Notebook:
* [2023-10-14 Entropy Intuition Visualisations.nb](https://www.wolframcloud.com/obj/5385d68a-17bc-4b69-b624-5ab9a15c106f)
* This notebook plots various things related to entropy for a binomial distribution (a coin toss)
* In particular look at houw the *summand function* is concave, and how you can change the *multiplier* away from the Shannon Entropy choice of -1*log(p) and yet entropy maximisation still works (still gives the same prioritisation of 50:50 odds as the max entropy and all points closer to 50:50 are greater entropy than those further, aaand [sic] the minimum entropy solution is p=0 or p=1)

## 3. Extension to more dimensions
It extends relatively easily. Where it gets interesting is when you constrain the **p** space. I want to explore this more and see whether the choice of *summand function*

# Other
Rough Notes
* Other kinds of entropy (Tsallis / RÃ©nyi)
